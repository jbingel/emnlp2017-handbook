SubmissionNumber#=%=#3
FinalPaperTitle#=%=#Unsupervised Text Normalization Using Distributed Representations of Words and Phrases
ShortPaperTitle#=%=#Unsupervised Text Normalization Using Distributed Representations of Words and Phrases
NumberOfPages#=%=#9
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Text normalization techniques that use rule-based normalization or string
similarity based on static dictionaries are typically unable to capture
domain-specific abbreviations (custy, cx → customer) and shorthands (5ever,
7ever → forever) used in informal texts. In this work, we exploit the
property that noisy and canonical forms of a particular word share similar
context in a large noisy text collection (millions or billions of social media
feeds from Twitter, Facebook, etc.). We learn distributed representations of
words to capture the notion of contextual similarity and subsequently learn
normalization lexicons from these representations in a completely unsupervised
manner. We experiment with linear and non-linear distributed representations
obtained from log-linear models and neural networks, respectively. We apply our
framework for normalizing customer care notes and Twitter. We also extend our
approach to learn phrase normalization lexicons (g2g → got to go) by training
distributed representations over compound words. Our approach outperforms
Microsoft Word, Aspell and a manually compiled urban dictionary from the Web
and achieves state-of-the-art results on a publicly available Twitter dataset.
Author{1}{Firstname}#=%=#Vivek Kumar
Author{1}{Lastname}#=%=#Rangarajan Sridhar
Author{1}{Email}#=%=#vivek_136@yahoo.com
Author{1}{Affiliation}#=%=#AT&T Labs - Research

==========